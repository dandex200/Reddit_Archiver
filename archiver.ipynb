{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ▄████████    ▄████████ ████████▄  ████████▄   ▄█      ███             ▄████████    ▄████████  ▄████████    ▄█    █▄     ▄█   ▄█    █▄     ▄████████    ▄████████\n",
    "#   ███    ███   ███    ███ ███   ▀███ ███   ▀███ ███  ▀█████████▄        ███    ███   ███    ███ ███    ███   ███    ███   ███  ███    ███   ███    ███   ███    ███\n",
    "#   ███    ███   ███    █▀  ███    ███ ███    ███ ███▌    ▀███▀▀██        ███    ███   ███    ███ ███    █▀    ███    ███   ███▌ ███    ███   ███    █▀    ███    ███\n",
    "#  ▄███▄▄▄▄██▀  ▄███▄▄▄     ███    ███ ███    ███ ███▌     ███   ▀        ███    ███  ▄███▄▄▄▄██▀ ███         ▄███▄▄▄▄███▄▄ ███▌ ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀\n",
    "# ▀▀███▀▀▀▀▀   ▀▀███▀▀▀     ███    ███ ███    ███ ███▌     ███          ▀███████████ ▀▀███▀▀▀▀▀   ███        ▀▀███▀▀▀▀███▀  ███▌ ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀\n",
    "# ▀███████████   ███    █▄  ███    ███ ███    ███ ███      ███            ███    ███ ▀███████████ ███    █▄    ███    ███   ███  ███    ███   ███    █▄  ▀███████████\n",
    "#   ███    ███   ███    ███ ███   ▄███ ███   ▄███ ███      ███            ███    ███   ███    ███ ███    ███   ███    ███   ███  ███    ███   ███    ███   ███    ███\n",
    "#   ███    ███   ██████████ ████████▀  ████████▀  █▀      ▄████▀          ███    █▀    ███    ███ ████████▀    ███    █▀    █▀    ▀██████▀    ██████████   ███    ███\n",
    "#   ███    ███                                                                         ███    ███                                                          ███    ███"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Archiver - An easy way to archive and view your Reddit Account. \n",
    "\n",
    "Downloads your profile info and posts/comments from your input reddit account. Creates a presentable webpage of your arhcived profile that can be saved locally. \n",
    "\n",
    "* Allows user to input desired Reddit Username and output directory for archive file via command line\n",
    "* Downloads user data using BeautifulSoup.\n",
    "* Saves data to JSON file\n",
    "* Loads JSON File into HTML webpage and saves webpage to desired directory\n",
    "\n",
    "\n",
    "By [dandex200](https://github.com/dandex200/) on Github\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from tkinter import filedialog, Tk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will save archive file to: /home/dandex/programming/The_Reddit_Archiver/Reddit_Archiver\n"
     ]
    }
   ],
   "source": [
    "#Username Input\n",
    "user_input = input(\"Enter desired Reddit Username to Archive:\")\n",
    "\n",
    "#Output Directory Input using Tkinter\n",
    "confirm_input = input(\"Choose Archive File Output Directory? (Y/N)\")\n",
    "\n",
    "#Select Output Directory if chosen\n",
    "if confirm_input.lower() == 'y':\n",
    "    root = Tk() \n",
    "    root.withdraw()\n",
    "    output_dir = filedialog.askdirectory()\n",
    "    root.destroy()\n",
    "    print(\"Will save archive file to: %s\" % output_dir)\n",
    "else:\n",
    "    print(\"No output directory selected - will save to current directory.\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "user_url = 'https://old.reddit.com/user/'+user_input\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "response = requests.get(user_url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "title_check = soup.title.text.split(' ')[0]\n",
    "\n",
    "if title_check == \"overview\":\n",
    "    print(\"working\")\n",
    "else:\n",
    "    print(\"User does not exist\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"username\": \"NPU-F\", \"comment_karma\": \"14,739\", \"post_karma\": \"27,091\"}\n"
     ]
    }
   ],
   "source": [
    "username = soup.title.text.split(' ')[2]\n",
    "\n",
    "comment_karma_box = soup.find('span', {'class': 'comment-karma'})\n",
    "comment_karma = comment_karma_box.text if comment_karma_box is not None else ''\n",
    "\n",
    "post_karma_box = soup.find('span', {'class': 'karma'})\n",
    "post_karma = post_karma_box.text if post_karma_box is not None else ''\n",
    "\n",
    "data = {'username': username, 'comment_karma': comment_karma, 'post_karma': post_karma}\n",
    "\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "print(json_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Posts & Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON FORMAT:\n",
    "#     Username:\n",
    "#         Post Karma:\n",
    "#         Comment Karma:\n",
    "#         Account Creation Date:\n",
    "#         Number of Submissions:\n",
    "#         Number of COmments:\n",
    "#     Posts:\n",
    "#         utc_date:\n",
    "#         Type:\n",
    "#         Subreddit:\n",
    "#         Title:\n",
    "#         File_Path:\n",
    "#         Permalink:\n",
    "#         Score:\n",
    "#     Comments:\n",
    "#         utc_date:\n",
    "#         Subreddit:\n",
    "#         Text:\n",
    "#         Score:\n",
    "#         Permalink:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theft of ATL airport funds by airport executives (corrected from the AJC) by [deleted] in Atlanta\n",
      "\n",
      "[–]NPU-F 31 points32 points33 points 3 days ago (0 children)Keisha Lance Bottoms made up a job at the airport for her campaign manager\n",
      "\n",
      "Marva Lewis, Bottoms’ campaign manager and one of the six people placed on the city payroll before Bottoms took office, was given the title of deputy general manager at the airport with a salary of $273,873 per year and never worked at the airport. Lewis, who became Bottoms’ chief of staff, held the airport job classification while leading the transition and for two weeks after the new mayor was sworn into office.\n",
      "\n",
      "\n",
      "permalinksavecontextfull comments (19)reportgive award\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "for comment in soup.find_all('div', class_='comment')[:1]:\n",
    "    for ps in comment:\n",
    "        print(ps.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "posts = []\n",
    "for post in soup.find_all('div', class_='link'):\n",
    "    print(\"2\")\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = {\n",
    "    'username': username,\n",
    "    'karma': {\n",
    "        'post_karma': post_karma,\n",
    "        'comment_karma': comment_karma\n",
    "    },\n",
    "    'posts': posts,\n",
    "    'comments': comments\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To-DO: PT1**\n",
    "* Successfully append comments, user info, and posts of first page to JSON in correct format\n",
    "* Save pictures/videos to directory\n",
    "* Present JSON and saved files in beginner format HTML webpage\n",
    "* Apply saving to all pages of reddit account\n",
    "* Add tests to ensure data is all saved (meets 25 posts per page, etc)\n",
    "* Create Readme on Github\n",
    "* Publish first time\n",
    "\n",
    "**TO-DO PT2**\n",
    "* Convert from notebook to script\n",
    "* Create GUI where you can choose if you want to save pics/videos, input user/output directory in GUI, etc\n",
    "* Apply ability to save pics/videos as choice\n",
    "* Create Pretty looking HTML archive page\n",
    "* Promote on Reddit\n",
    "* Release as second update \n",
    "\n",
    "**TO-DO PT3**\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
