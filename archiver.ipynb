{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ▄████████    ▄████████ ████████▄  ████████▄   ▄█      ███             ▄████████    ▄████████  ▄████████    ▄█    █▄     ▄█   ▄█    █▄     ▄████████    ▄████████\n",
    "#   ███    ███   ███    ███ ███   ▀███ ███   ▀███ ███  ▀█████████▄        ███    ███   ███    ███ ███    ███   ███    ███   ███  ███    ███   ███    ███   ███    ███\n",
    "#   ███    ███   ███    █▀  ███    ███ ███    ███ ███▌    ▀███▀▀██        ███    ███   ███    ███ ███    █▀    ███    ███   ███▌ ███    ███   ███    █▀    ███    ███\n",
    "#  ▄███▄▄▄▄██▀  ▄███▄▄▄     ███    ███ ███    ███ ███▌     ███   ▀        ███    ███  ▄███▄▄▄▄██▀ ███         ▄███▄▄▄▄███▄▄ ███▌ ███    ███  ▄███▄▄▄      ▄███▄▄▄▄██▀\n",
    "# ▀▀███▀▀▀▀▀   ▀▀███▀▀▀     ███    ███ ███    ███ ███▌     ███          ▀███████████ ▀▀███▀▀▀▀▀   ███        ▀▀███▀▀▀▀███▀  ███▌ ███    ███ ▀▀███▀▀▀     ▀▀███▀▀▀▀▀\n",
    "# ▀███████████   ███    █▄  ███    ███ ███    ███ ███      ███            ███    ███ ▀███████████ ███    █▄    ███    ███   ███  ███    ███   ███    █▄  ▀███████████\n",
    "#   ███    ███   ███    ███ ███   ▄███ ███   ▄███ ███      ███            ███    ███   ███    ███ ███    ███   ███    ███   ███  ███    ███   ███    ███   ███    ███\n",
    "#   ███    ███   ██████████ ████████▀  ████████▀  █▀      ▄████▀          ███    █▀    ███    ███ ████████▀    ███    █▀    █▀    ▀██████▀    ██████████   ███    ███\n",
    "#   ███    ███                                                                         ███    ███                                                          ███    ███"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Archiver - An easy way to archive and view your Reddit Account. \n",
    "\n",
    "Downloads your profile info and posts/comments from your input reddit account. Creates a presentable webpage of your arhcived profile that can be saved locally. \n",
    "\n",
    "* Allows user to input desired Reddit Username and output directory for archive file via command line\n",
    "* Downloads user data using BeautifulSoup.\n",
    "* Saves data to JSON file\n",
    "* Loads JSON File into HTML webpage and saves webpage to desired directory\n",
    "\n",
    "\n",
    "By [dandex200](https://github.com/dandex200/) on Github\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from tkinter import filedialog, Tk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Username Input\n",
    "user_input = input(\"Enter desired Reddit Username to Archive:\")\n",
    "\n",
    "#Output Directory Input using Tkinter\n",
    "confirm_input = input(\"Choose Archive File Output Directory? (Y/N)\")\n",
    "\n",
    "if confirm_input.lower() == 'y':\n",
    "    root = Tk() \n",
    "    root.withdraw()\n",
    "    output_dir = filedialog.askdirectory()\n",
    "    root.destroy()\n",
    "\n",
    "else:\n",
    "    print(\"No output directory selected - will save to current directory.\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"username\": \"NPU-F\", \"comment_karma\": \"14,739\", \"post_karma\": \"27,087\"}\n"
     ]
    }
   ],
   "source": [
    "#Test BOX\n",
    "\n",
    "url = 'https://old.reddit.com/user/NPU-F' \n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# extract the username from the page title\n",
    "username = soup.title.text.split(' ')[2]\n",
    "\n",
    "# extract the comment karma from the user info box\n",
    "comment_karma_box = soup.find('span', {'class': 'comment-karma'})\n",
    "comment_karma = comment_karma_box.text if comment_karma_box is not None else ''\n",
    "\n",
    "# extract the post karma from the user info box\n",
    "post_karma_box = soup.find('span', {'class': 'karma'})\n",
    "post_karma = post_karma_box.text if post_karma_box is not None else ''\n",
    "\n",
    "# create a dictionary with the scraped data\n",
    "data = {'username': username, 'comment_karma': comment_karma, 'post_karma': post_karma}\n",
    "\n",
    "# output the dictionary to JSON format\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# print the JSON data to the console\n",
    "print(json_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
